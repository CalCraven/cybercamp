{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define some data as a simple dictionary and then convert it to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = { \n",
    "\t'run1': [360, 0.91, 20.1],\n",
    "\t'run2': [305, 0.98, 22.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can very trivially convert this to a pandas dataframe.  We can also pass lables via \"index\", to tell us what each of the entries means and allow us to more easily search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame(data_dict, index=['T', 'S2', 'angle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily output this as a nice table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the print function it is basically the same, just not quite as nice looking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how to extract info.  To get info for a single entry in the dict, can easily just use the name as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['run1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the Temperature info from each of the runs, we access the data a little bit differently, using the \"loc\" command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.loc['T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two can be combined to access, e.g., the temperature of run1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['run1'].loc['T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, since T is the first entry, can access it just using an integer index of 0, or pass the index label (since we defined one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['run1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['run1']['T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to write dataframes to CSV files (and read them).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv(r'test_data.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_from_csv = pd.read_csv('test_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_from_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now read in some real data as a CSV file.  This is the number of  COVID-19 cases in the state of Tennessee per day from tn.gov [Daily Case Information](https://www.tn.gov/health/cedep/ncov/data/downloadable-datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenn_data = pd.read_csv('datasets/Tenn_Pandemic_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see by the way this has been imported, the dictionary keys are 'DATE', 'TOTAL_CASES', 'NEW_CASES', and etc., so we can use these to extract out specific information.  For example, let's plot  'DATE' and 'TOTAL_CASES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tenn_data['DATE'], tenn_data['TOTAL_CASES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather difficult to read that x-axis, let's do some quick formatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first let's make a copy of the data that gives the date/time of update\n",
    "formatted_date = tenn_data['DATE'][:].copy()\n",
    "#next, let's only keep the date, discarding the time, this is just the first\n",
    "#10 characters of the string\n",
    "formatted_date = [date.replace(date[10:], '') for date in formatted_date]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#to be able to read the labels, we'll use a built-in function to tilt them\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# and then define the number of tick markers to show a more manageable set\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "ax.plot(formatted_date, tenn_data['TOTAL_CASES'])\n",
    "ax.invert_xaxis()\n",
    "\n",
    "plt.ylabel('number of positive cases')\n",
    "plt.xlabel('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's rather easy to just restrict the data range we plot, e.g., let's just pick out data from the \"beginning\" where the number of cases was most rapidly growing (the end of this window roughly corresponds  with the end of the first wave, around 07-31-20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#to be able to read the labels, we'll use a built in function to tilt them\n",
    "fig.autofmt_xdate()\n",
    "# and then define the number of tick markers to show\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "ax.plot(formatted_date[300:-1], tenn_data['TOTAL_CASES'][300:-1])\n",
    "plt.ylabel('number of positive cases')\n",
    "ax.invert_xaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us  use the scipy optimize routine to fit this region, so we can interpolate the initial exponential growth. For more information of scipy, follow this [link.](https://docs.scipy.org/doc/scipy/reference/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_func(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of 'N' points from 1 to 0 corresponding to the number of entrees\n",
    "# This is necessary since the x-axis is dates rather than floats, and is in a reversed order\n",
    "\n",
    "xdata = np.linspace(1, 0, len(tenn_data['TOTAL_CASES']))\n",
    "\n",
    "#fit the curve, but limit to that middle data above\n",
    "popt, pcov = curve_fit(exp_func, xdata[300:-1], tenn_data['TOTAL_CASES'][300:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curve fit doesn't give you an R-squared value by default, so we just need to do a few quick calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = tenn_data['TOTAL_CASES'][300:-1]- exp_func(xdata[300:-1], *popt)\n",
    "#calculate residual sum of squares\n",
    "ss_res = np.sum(residuals**2)\n",
    "#get the total sum of squares\n",
    "ss_tot = np.sum((tenn_data['TOTAL_CASES'][300:-1]-np.mean(tenn_data['TOTAL_CASES'][300:-1]))**2)\n",
    "#get the r-squared value\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot our fit to get an idea of where it fits the total_cases_information outside of that region. If you run the cell below, you should see that the initial exponential growth overpredicts the total cases further into 2020 and 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "#to be able to read the labels, we'll use a built-in function to tilt them\n",
    "fig.autofmt_xdate()\n",
    "# and then define the number of tick markers to show\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "#plot all of the data\n",
    "ax.plot(formatted_date, tenn_data['TOTAL_CASES'], marker='o', ls='')\n",
    "ax.invert_xaxis()\n",
    "\n",
    "#plot of the fit of the middle region\n",
    "#to plot the fit, we'll just pass the function we have our x_data points and the fitted a,b,c values\n",
    "# that are saved in the popt area\n",
    "ax.plot(formatted_date,  exp_func(xdata, *popt), 'r--')\n",
    "\n",
    "plt.ylabel('number of positive cases')\n",
    "plt.xlabel('date')\n",
    "\n",
    "#change to a log scale\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below gives an alternative way to evaluate our exponential fit. A value of 1 means the prediction matched the actual data perfectly. A value of less than one means we overpredicted the actual data. Looking at plots like this can show you if your fit is done well. There should be randomly dispersed scatter both over and under one. In this case, our fit stops performing evenly remotely acceptably outside of our fit domain (April 2020 to August 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.autofmt_xdate()\n",
    "# and then define the number of tick markers to show\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "#plot all of the data\n",
    "ax.plot(formatted_date[0:-1],  tenn_data['TOTAL_CASES'][0:-1]/exp_func(xdata[0:-1], *popt), 'o', ls='--')\n",
    "ax.invert_xaxis()\n",
    "\n",
    "plt.ylabel('actual/predicted postitive cases')\n",
    "plt.xlabel('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's been over a year that we've been dealing with the pandemic. How do our infection rates compare to last year? We can do this by using the Get_Rate function to evaluate the number of new infections each day. The Reverse function will also help get our data in the forward orientation through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reverse(it_object):\n",
    "    lst = list(it_object)\n",
    "    lst.reverse()\n",
    "    return lst\n",
    "\n",
    "def Get_Rate(series):\n",
    "    lst = list(series)\n",
    "    delta_cases = []\n",
    "    for i in np.arange(0,len(lst[1:])):\n",
    "        delta_cases.append(int(lst[i+1]-lst[i]))\n",
    "    return delta_cases\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.autofmt_xdate()\n",
    "# and then define the number of tick markers to show\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "# and then define the number of tick markers to show\n",
    "\n",
    "#plot all of the data\n",
    "ax.plot(Reverse(list(formatted_date[1:31])),  \n",
    "        Get_Rate(Reverse(tenn_data['TOTAL_CASES'][0:31])), 'o', ls='--', \n",
    "        color='purple', label='This Year')\n",
    "ax.plot(Reverse(list(formatted_date[1:31])),  \n",
    "        Get_Rate(Reverse(tenn_data['TOTAL_CASES'][365:396])), 'o', ls='--',\n",
    "        color='blue', label='Last Year' )\n",
    "\n",
    "\n",
    "plt.ylabel('Covid cases for this year compared to last year')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most obvious differences between this year and last year is the introduction of the free vaccine in the state. However, since this year we are still getting more positive cases than we had last year, we may be led to conclude that the vaccine is not having any effect or even making the number of cases worse! Let's take a closer statistical look at this correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick statistical analysis concerning the effect of vaccines on our total_cases. Here's some vaccine data from https://github.com/owid/covid-19-data/tree/master/public/data/vaccinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "vac_data = pd.read_csv('datasets/us_state_vaccinations.csv')\n",
    "# Parse out data just for Tennessee\n",
    "tenn_vac_data = vac_data[vac_data['location']=='Tennessee']\n",
    "# We will look at this dataset for people considered fully vaccinated\n",
    "print(len(tenn_vac_data['people_fully_vaccinated']))\n",
    "\n",
    "#first let's make a copy of the data that gives the date/time of update\n",
    "formatted_vac_date = tenn_vac_data['date'][:].copy()\n",
    "\n",
    "#plot the vaccinations\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#to be able to read the labels, we'll use a built-in function to tilt them\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# and then define the number of tick markers to show a more manageable set\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n",
    "\n",
    "ax.plot(formatted_vac_date, tenn_vac_data['people_fully_vaccinated'])\n",
    "\n",
    "plt.ylabel('number of fully vaccinated people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we now have vaccination data for the last 140 or so days. If we align this with our total_cases_data, we can get an idea of the correlation between the two. Let's take a look at this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dates are off, we need to cut out a few dates so the data matches for each date.\n",
    "# We'll take our data from 02-01-21 to 05-25-21\n",
    "# We also need to do some work to remove a missing value from our dataset. We don't have vaccine \n",
    "# information for the 34th day, so we'll remove that day from the positive_cases_data. \n",
    "\n",
    "# Let's grab that Reverse function again\n",
    "def Reverse(it_object):\n",
    "    lst = list(it_object)\n",
    "    lst.reverse()\n",
    "    return lst\n",
    "\n",
    "# Let's make sure we're grabbing data from the right dates. \n",
    "#This will be printed out at the end to compare with the total_cases_dates\n",
    "formatted_vac_date = tenn_vac_data['date'][:].copy()\n",
    "formatted_vac_date = list(formatted_vac_date[20:-3])\n",
    "# this will turn our values into integers to make them easier to work with\n",
    "vac_data = list((tenn_vac_data['people_fully_vaccinated'][20:-3]))\n",
    "for i,value in enumerate(vac_data.copy()):\n",
    "    # check to make sure values are nan\n",
    "    if not pd.isna(vac_data[i]):\n",
    "        vac_data[i] = int(value)\n",
    "# The 14th value (34th in the original dataset) needs to get removed since it doesn't have a value\n",
    "vac_data.pop(14)\n",
    "vac_data = np.array(vac_data)\n",
    "\n",
    "# Similar dataworkup for the total_cases information from before.\n",
    "formatted_cases_date = Reverse(list(formatted_date[0:116]))\n",
    "tenn_cases_data = list(Reverse(list(tenn_data['TOTAL_CASES'][0:116])))\n",
    "tenn_cases_data.pop(14)\n",
    "tenn_cases_data = np.array(tenn_cases_data)\n",
    "\n",
    "formatted_vac_date[0],formatted_cases_date[0],formatted_vac_date[-1],formatted_cases_date[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(vac_data[1:], Get_Rate(tenn_cases_data),':')\n",
    "\n",
    "plt.xlabel('number of fully vaccinated people')\n",
    "plt.ylabel('number of new positive covid cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some linear interpolation for this correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fitted_Line(x,a,b):\n",
    "    return a * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the curve to the Get_Rate information from tenn_cases_data\n",
    "# The extra information is to give the solver method some better places to guess.\n",
    "popt, pcov = curve_fit(Fitted_Line, vac_data[1:], Get_Rate(tenn_cases_data),\n",
    "                       p0=[-0.001,2000], check_finite=True, \n",
    "                       method='trf',bounds = ([-0.002,0],[0,4000]))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#plot all of the data\n",
    "ax.plot(vac_data[1:], Get_Rate(tenn_cases_data),':')\n",
    "\n",
    "#to plot the fit, we'll just pass the function we have our vaccine data points and the fitted a,b values\n",
    "# that are saved in the popt area\n",
    "ax.plot(vac_data[1:],  Fitted_Line(vac_data[1:], *popt), 'r--')\n",
    "\n",
    "plt.ylabel('number of positive cases')\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some statistics on our function: daily_cases = -5.9e-4 * vaccinated + 1.8e3 <br>\n",
    "H0: slope = 0 <br>\n",
    "H1: slope < 0 <br>\n",
    "\n",
    "For this we need to apply linear regression statistics to get the uncertainty in our parameter a (the slope). <br><br>\n",
    "\n",
    "uncertainty = t(alpha, df=n-2) * standard deviation <br>\n",
    "standard deviation of slope = sqrt( sy,x^2 / SSxx )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#degrees of freedom\n",
    "df = len(vac_data[1:]) - 2\n",
    "# solve for varxy standard deviation of y(x)\n",
    "varyx = np.sum((Get_Rate(tenn_cases_data) - Fitted_Line(vac_data[1:], *popt))**2) * df**-1\n",
    "\n",
    "# solve for SSxx\n",
    "SSxx = np.sum((vac_data[1:] - vac_data[1:].mean())**2)\n",
    "    \n",
    "t = stats.t.ppf(1-0.025, df)\n",
    "print('The 95% confidence of the slope is {0:.1e} +- {1:.1e} cases/vaccinations'.format(popt[0], \n",
    "                                                                                        t * np.sqrt(varyx / SSxx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the slope is significantly different from 0, we can reject our null hypothesis and support the alternative hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stepping back to where we started, the dictionaries allow us to easily manage our data space and keep track of lots of different pieces of information that can be easily iterated over.\n",
    "\n",
    "Let's create some totally fictuious data for T and PE for two different runs (data that most likely would be read in from a simulation energy file or the result of analysis by a code and wouldn't be defined by hand).  We will then make a dataframe for each run and then put these in a dictionary.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run1_data = { \n",
    "\t'T': [300, 305, 310, 315, 310, 315, 320, 325, 320, 315, 310, 315],\n",
    "\t'PE': [1489, 1523, 1649, 1554, 1634, 1780, 1900, 1843, 1724, 1652, 1400, 1323]\n",
    "}\n",
    "\n",
    "run2_data = { \n",
    "\t'T': [300, 305, 305, 310, 315, 320, 325, 320, 325, 320, 315, 320],\n",
    "\t'PE': [1482, 1512, 1432, 1623, 1723, 1849, 1948, 2200, 2129, 2003, 1802, 1938]\n",
    "}\n",
    "\n",
    "r1_pd = pd.DataFrame(run1_data)\n",
    "r2_pd = pd.DataFrame(run2_data)\n",
    "\n",
    "\n",
    "sim_data_dict = {'run1': r1_pd,  'run2': r2_pd}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a dictionary we can again, hone in on specific pieces of information, like e.g., only run2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_dict['run2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sim in sim_data_dict:\n",
    "    plt.plot(sim_data_dict[sim]['T'], label=sim)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of course not the only way to define a dataspace.  This just happens to be a way I personally like.  \n",
    "\n",
    "E.g., instead of having a dictionary be the top level container, we could put the run1_data and run2_data dictionaries into a dictionary, then convert that to a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_dict2 = {'run1': run1_data,  'run2': run2_data}\n",
    "\n",
    "sim_df = pd.DataFrame(sim_data_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sim in sim_df:\n",
    "    plt.plot(sim_df[sim]['T'], label=sim)\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some built in functions that make things easy to get information quickly out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sim_data_dict['run1']['T'].mean(), '+/-', sim_data_dict['run1']['T'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily export to a numpy array as well.  A quick note, pandas uses the Bessel's correction in the standard deviation formulat. That is N-1, rather than N.  So this will give a slightly different value than numpy.std(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_array = sim_data_dict['run1']['T'].to_numpy()\n",
    "print(T_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T_array.mean(), '+/-', T_array.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
